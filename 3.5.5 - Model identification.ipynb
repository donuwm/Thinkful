{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction - \n",
    "Identify which supervised learning method(s) would be best for addressing that particular problem and explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.\n",
    " - OLS:  the predicted value does not reside in the training set (history of last 20 Olpympics).  Also, we are predicting a time which is a continuous variable and not categorical.  \n",
    "\n",
    "2.  You have more features (columns) than rows in your dataset.  \n",
    " - Possibly Lasso regression to reduce many of your features to zero, thus eliminating features (depending on lambda value).  SVM might also be good here to prevent overfitting from all of the features.\n",
    "\n",
    "3.  Identify the most important characteristic predicting likelihood of being jailed before age 20.  \n",
    " - Use PCA (with n_predictors=1) or Lasso regression.  Another option would be to use multivariable regression and observe the highest coeffecient for each variable which would correspond to the most significant characteristic (after normalizing all features).  \n",
    "\n",
    "4.  Implement a filter to “highlight” emails that might be important to the recipient  \n",
    " - Naive Bayes.  This is a binary (Bernoulli) classification problem: important or not important.  \n",
    "\n",
    "5.  You have 1000+ features.  \n",
    " - Us Lasso to reduce features to zero.\n",
    "\n",
    "6.  Predict whether someone who adds items to their cart on a website will purchase the items.  \n",
    " - Naive Bayes since it is a binary cliassifier prediction.  KNN classifier might be good too; the 'neighbors' being other items in the current cart or other items in the cart from previous purchases by this same user.\n",
    "\n",
    "7.  Your dataset dimensions are 982400 x 500  \n",
    " - Decision Tree to save on processing time and memory constraints.\n",
    "\n",
    "8.  Identify faces in an image.  \n",
    " - Gradient boosting:  perform an analysis and then focus on the errors of the decision tree to perform another pass in order to reduce errors and correctly classify the person.\n",
    "\n",
    "9.  Predict which of three flavors of ice cream will be most popular with boys vs girls.  \n",
    " - Multinomia Naive Bayes: there are 3 categorical outcomes (each of the 3 flavors) and only 2 features (boys and girls).  However, the genders may not be independent.  Random Forest might be good because of the small amount of data and the prediction value is definitely within our sample space.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
