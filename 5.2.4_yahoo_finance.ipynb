{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code was copied from ....\n",
    "[https://www.promptcloud.com/blog/how-to-scrape-yahoo-finance-data-using-python/]\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "# For ignoring SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PRESENT_VALUE': '10.02', 'OTHER_DETAILS': {'DIVIDEND_AND_YIELD': '0.60 (6.30%)'}}\n",
      "----------Extraction of data is complete. Check json file.----------\n"
     ]
    }
   ],
   "source": [
    "# Input from the user\n",
    "# url = input('Enter Yahoo Finance Company Url- ')\n",
    "# url = input('https://finance.yahoo.com/quote/F?p=F&.tsrc=fin-srch')\n",
    "url = 'https://finance.yahoo.com/quote/F'\n",
    "\n",
    "# Making the website believe that you are accessing it using a Mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the HTML page for easy extraction of data.\n",
    "\n",
    "soup = BeautifulSoup(webpage, 'html.parser')\n",
    "html = soup.prettify('utf-8')\n",
    "company_json = {}\n",
    "other_details = {}\n",
    "for span in soup.findAll('span',\n",
    "                         attrs={'class': 'Trsdu(0.3s) Trsdu(0.3s) Fw(b) Fz(36px) Mb(-4px) D(b)'\n",
    "                         }):\n",
    "    company_json['PRESENT_VALUE'] = span.text.strip()\n",
    "# for div in soup.findAll('div', attrs={'class': 'D(ib) Va(t)'}):\n",
    "#     for span in div.findAll('span', recursive=False):\n",
    "#         company_json['PRESENT_GROWTH'] = span.text.strip()\n",
    "# for td in soup.findAll('td', attrs={'data-test': 'PREV_CLOSE-value'}):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['PREV_CLOSE'] = span.text.strip()\n",
    "# for td in soup.findAll('td', attrs={'data-test': 'OPEN-value'}):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['OPEN'] = span.text.strip()\n",
    "# for td in soup.findAll('td', attrs={'data-test': 'BID-value'}):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['BID'] = span.text.strip()\n",
    "# for td in soup.findAll('td', attrs={'data-test': 'ASK-value'}):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['ASK'] = span.text.strip()\n",
    "# for td in soup.findAll('td', attrs={'data-test': 'DAYS_RANGE-value'}):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['DAYS_RANGE'] = span.text.strip()\n",
    "# for td in soup.findAll('td',\n",
    "#                        attrs={'data-test': 'FIFTY_TWO_WK_RANGE-value'}):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['FIFTY_TWO_WK_RANGE'] = span.text.strip()\n",
    "# for td in soup.findAll('td', attrs={'data-test': 'TD_VOLUME-value'}):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['TD_VOLUME'] = span.text.strip()\n",
    "# for td in soup.findAll('td',\n",
    "#                        attrs={'data-test': 'AVERAGE_VOLUME_3MONTH-value'\n",
    "#                        }):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['AVERAGE_VOLUME_3MONTH'] = span.text.strip()\n",
    "# for td in soup.findAll('td', attrs={'data-test': 'MARKET_CAP-value'}):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['MARKET_CAP'] = span.text.strip()\n",
    "# for td in soup.findAll('td', attrs={'data-test': 'BETA_3Y-value'}):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['BETA_3Y'] = span.text.strip()\n",
    "# for td in soup.findAll('td', attrs={'data-test': 'PE_RATIO-value'}):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['PE_RATIO'] = span.text.strip()\n",
    "# for td in soup.findAll('td', attrs={'data-test': 'EPS_RATIO-value'}):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['EPS_RATIO'] = span.text.strip()\n",
    "# for td in soup.findAll('td', attrs={'data-test': 'EARNINGS_DATE-value'\n",
    "#                        }):\n",
    "#     other_details['EARNINGS_DATE'] = []\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['EARNINGS_DATE'].append(span.text.strip())\n",
    "for td in soup.findAll('td',\n",
    "                       attrs={'data-test': 'DIVIDEND_AND_YIELD-value'}):\n",
    "    other_details['DIVIDEND_AND_YIELD'] = td.text.strip()\n",
    "# for td in soup.findAll('td',\n",
    "#                        attrs={'data-test': 'EX_DIVIDEND_DATE-value'}):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['EX_DIVIDEND_DATE'] = span.text.strip()\n",
    "# for td in soup.findAll('td',\n",
    "#                        attrs={'data-test': 'ONE_YEAR_TARGET_PRICE-value'\n",
    "#                        }):\n",
    "#     for span in td.findAll('span', recursive=False):\n",
    "#         other_details['ONE_YEAR_TARGET_PRICE'] = span.text.strip()\n",
    "company_json['OTHER_DETAILS'] = other_details\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(company_json, outfile, indent=4)\n",
    "print(company_json)\n",
    "with open('output_file.html', 'wb') as file:\n",
    "    file.write(html)\n",
    "print('----------Extraction of data is complete. Check json file.----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy  \n",
    "\n",
    "Still testing this... if look like Yahoo may have changed attributes of their web page as it pertains to API's and scraping.  Not sure what the problem is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Selector xpath='//td' data='<td class=\"C(black) W(51%)\" data-reactid'>\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class YHSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"YH\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://finance.yahoo.com/quote/F/',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for stock in response.xpath('//td'):\n",
    "            print(stock)\n",
    "#             print(stock.extract())\n",
    "\n",
    "            print(stock.xpath('[@class=\"Ta(end) Fw(600) Lh(14px)\"]').extract())\n",
    "            \n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield { \n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "#                 'test': stock.xpath(\"text()\").extract_first(),\n",
    "                'dividend': stock.xpath('data-test[@class=\"DIVIDEND_AND_YIELD-value\"]/text()').extract_first()\n",
    "#                 'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "#                 'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "#                 'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "        \n",
    "# test_inst = YHSpider(scrapy.Spider)\n",
    "# YHSpider.parse(response)\n",
    "        \n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'firstpage.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(YHSpider)\n",
    "process.start()\n",
    "print('Success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected character found when decoding 'false'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-41da824473a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfirstpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'firstpage.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'records'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirstpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirstpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m             obj = self._get_object_parser(\n\u001b[1;32m--> 534\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m             )\n\u001b[0;32m    536\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'frame'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'series'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    883\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m--> 885\u001b[1;33m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_process_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected character found when decoding 'false'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "firstpage = pd.read_json('firstpage.json', orient='records', lines=True)\n",
    "print(firstpage.shape)\n",
    "print(firstpage.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
